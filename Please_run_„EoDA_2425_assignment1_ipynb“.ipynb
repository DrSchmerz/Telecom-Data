{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrSchmerz/Telecom-Data/blob/main/Please_run_%E2%80%9EEoDA_2425_assignment1_ipynb%E2%80%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "3d64f0fd-710e-48c7-8e5b-e56c877e3f79",
          "showTitle": false,
          "title": ""
        },
        "id": "pVCR4GznXPBd"
      },
      "source": [
        "# Engineering of Data Analysis: assignment 1\n",
        "\n",
        "By delivering this notebook, we confirm that the code presented was developed by the following students.\n",
        "\n",
        "**Student num:66286     ; Name:Hannes Reinhardt\n",
        "\n",
        "**Student num:64737     ; Name: Philipp Oliver Laurence Goetting\n",
        "\n",
        "**DEADLINE:** 21st April, 23h59\n",
        "\n",
        "**Only one student should deliver the notebook**\n",
        "\n",
        "\n",
        "**IMPORTANT:** To use GPUs in Colab, you need to go to the menu ```Edit > Notebook settings``` and select GPU as the hardware accelerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "51823ee2-b2f5-4933-b3a3-3f52853b5339",
          "showTitle": false,
          "title": ""
        },
        "id": "xyM6HefJXPBe"
      },
      "source": [
        "Some useful links:\n",
        "* [ACM DEBS 2015 Grand Challenge](http://www.debs2015.org/call-grand-challenge.html)\n",
        "\n",
        "* [Spark web site](https://spark.apache.org/)\n",
        "\n",
        "* [Spark MLlib main page](https://spark.apache.org/mllib/)\n",
        "* [Spark MLlib guide](https://spark.apache.org/docs/latest/ml-guide.html)\n",
        "\n",
        "* [CuDF documentation](https://docs.rapids.ai/api/cudf/stable/)\n",
        "* [cuML documentation](https://docs.rapids.ai/api/cuml/stable/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install software"
      ],
      "metadata": {
        "id": "MhyO9-O2wuoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Software to download files\n",
        "!pip install gdown"
      ],
      "metadata": {
        "id": "zrxxQrrv0nfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fe0cb6-e4a1-46d1-8c5d-1b5e266a1ae3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Spark\n",
        "!apt-get install openjdk-17-jdk-headless\n",
        "!pip install pyspark==4.0.0.dev2\n",
        "!mkdir checkpoint\n"
      ],
      "metadata": {
        "id": "uDH5ObnQwtRy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c432df7-9547-46ba-c8b2-34c8cb2204cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-17-jdk-headless is already the newest version (17.0.14+7-1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Requirement already satisfied: pyspark==4.0.0.dev2 in /usr/local/lib/python3.11/dist-packages (4.0.0.dev2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==4.0.0.dev2) (0.10.9.7)\n",
            "mkdir: cannot create directory ‘checkpoint’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install RAPIDS - for using cuDF, cuML\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible,\n",
        "# it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n"
      ],
      "metadata": {
        "id": "q7vItADfw9JS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "859e2884-5435-486b-de81-8ee0bd5e4515"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'rapidsai-csp-utils' already exists and is not an empty directory.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pynvml.py\", line 2415, in _LoadNvmlLibrary\n",
            "    nvmlLib = CDLL(\"libnvidia-ml.so.1\")\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OSError: libnvidia-ml.so.1: cannot open shared object file: No such file or directory\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/rapidsai-csp-utils/colab/pip-install.py\", line 18, in <module>\n",
            "    pynvml.nvmlInit()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pynvml.py\", line 2387, in nvmlInit\n",
            "    nvmlInitWithFlags(0)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pynvml.py\", line 2370, in nvmlInitWithFlags\n",
            "    _LoadNvmlLibrary()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pynvml.py\", line 2417, in _LoadNvmlLibrary\n",
            "    _nvmlCheckReturn(NVML_ERROR_LIBRARY_NOT_FOUND)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pynvml.py\", line 1042, in _nvmlCheckReturn\n",
            "    raise NVMLError(ret)\n",
            "pynvml.NVMLError_LibraryNotFound: NVML Shared Library Not Found\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/rapidsai-csp-utils/colab/pip-install.py\", line 20, in <module>\n",
            "    raise Exception(\"\"\"\n",
            "Exception: \n",
            "                  Unfortunately you're in a Colab instance that doesn't have a GPU.\n",
            "\n",
            "                  Please make sure you've configured Colab to request a GPU Instance Type.\n",
            "\n",
            "                  Go to 'Runtime -> Change Runtime Type --> under the Hardware Accelerator, select GPU', then try again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aa7FGQcNEkpu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "The data sets are available in the following link: https://drive.google.com/drive/folders/1WMwLUj0t4Q0GSll96lbF2bDjaPVh1w8z?usp=sharing. For running in Google Colab, you should access the link and Add Shortcut to your Drive.\n",
        "\n"
      ],
      "metadata": {
        "id": "bc9cRxyLxzDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyspark pandas numpy matplotlib scikit-learn cudf cupy cuml"
      ],
      "metadata": {
        "id": "3AswSLoZIMkg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import *\n",
        "from pyspark.ml.feature import *\n",
        "import pyspark.pandas as ps\n",
        "\n",
        "import timeit\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.cluster\n",
        "\n",
        "import cudf\n",
        "import cupy as cp\n",
        "import cuml.cluster\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "_PbXQMMwIIDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba40385c-2039-4136-baa4-91eed5ed30a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/pandas/__init__.py:43: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/cudf/utils/_ptxcompiler.py:64: UserWarning: Error getting driver and runtime versions:\n",
            "\n",
            "stdout:\n",
            "\n",
            "\n",
            "\n",
            "stderr:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 4, in <module>\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 314, in __getattr__\n",
            "    raise CudaSupportError(\"Error at driver init: \\n%s:\" %\n",
            "numba.cuda.cudadrv.error.CudaSupportError: Error at driver init: \n",
            "\n",
            "CUDA driver library cannot be found.\n",
            "If you are sure that a CUDA driver is installed,\n",
            "try setting environment variable NUMBA_CUDA_DRIVER\n",
            "with the file path of the CUDA driver shared library.\n",
            ":\n",
            "\n",
            "\n",
            "Not patching Numba\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/cudf/utils/gpu_utils.py:62: UserWarning: Failed to dlopen libcuda.so.1\n",
            "  warnings.warn(str(e))\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except ValueError:\n",
        "    print(\"Google Drive is already mounted at /content/drive\")\n",
        "\n",
        "# Large dataset - 12.3 GB compressed\n",
        "#FILENAME = \"/content/drive/MyDrive/assignment1/sorted_data.csv.gz\"\n",
        "\n",
        "# Small dataset - 135 MB compressed\n",
        "#FILENAME = \"/content/drive/MyDrive/assignment1/sample.csv.gz\"\n",
        "\n",
        "# Tiny dataset - 6.8 MB compressed (you can use this one for speeding up development only)\n",
        "FILENAME = \"/content/drive/MyDrive/assignment1/tiny.csv.gz\"\n",
        "\n",
        "# (Optional) Verify the file exists, to provide user feedback\n",
        "if not os.path.exists(FILENAME):\n",
        "    print(f\"Error: Dataset file not found at {FILENAME}. \")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOVr6m3RQAGP",
        "outputId": "b47c99a4-dc39-449f-ab4b-2aadc95ac40a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "d9eb532f-aee3-4732-8db8-9b627a785c47",
          "showTitle": false,
          "title": ""
        },
        "id": "RAm-BRKXXPBg"
      },
      "source": [
        "## Exercise 1: simple statistics\n",
        "\n",
        "Compute, for each license, the total amount of money collected.\n",
        "\n",
        "Comapre the time it takes to execute the code for Pandas, Spark Pandas API, Spark SQL and cuDF.\n",
        "\n",
        "**Draw some conclusions** by comparing the time for performing the computation using Spark and Pandas, and also when using the small and long dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "fbceb57e-91d7-430a-8406-ad44dec1b38a",
          "showTitle": false,
          "title": ""
        },
        "id": "I--Vmbx8XPBg"
      },
      "source": [
        "### Code: Pandas library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "2708ac4e-177f-4c8b-a7c3-b9ebe08a55f2",
          "showTitle": false,
          "title": ""
        },
        "id": "khxF5l8VXPBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3410f8e6-49a4-46b6-be33-218df50f408a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime = 0.8054232597351074\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "mySchema = [\"medallion\", \"hack_license\", \"pickup_datetime\",\n",
        "            \"dropoff_datetime\", \"trip_time_in_secs\", \"trip_distance\",\n",
        "            \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\n",
        "            \"dropoff_latitude\", \"payment_type\", \"fare_amount\",\n",
        "            \"surcharge\", \"mta_tax\", \"tip_amount\",\n",
        "            \"tolls_amount\", \"total_amount\"]\n",
        "\n",
        "dataset = pd.read_csv(FILENAME,names=mySchema)\n",
        "result = dataset[[\"hack_license\",\"total_amount\"]].groupby(\"hack_license\").sum()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print( \"Runtime = \" + str(end_time - start_time))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "d81bf814-b05b-4e88-9d6b-b31cbff8a580",
          "showTitle": false,
          "title": ""
        },
        "id": "C6cgoZCdXPBg"
      },
      "source": [
        "### Results (Pandas)\n",
        "\n",
        "The time to process the small dataset was : **was 8.62111210823059** seconds.\n",
        "\n",
        "The time to process the large dataset was : **TO COMPLETE** seconds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "fbceb57e-91d7-430a-8406-ad44dec1b38a",
          "showTitle": false,
          "title": ""
        },
        "id": "fn_RZT6s8TaM"
      },
      "source": [
        "### Code: Spark Pandas API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "2708ac4e-177f-4c8b-a7c3-b9ebe08a55f2",
          "showTitle": false,
          "title": ""
        },
        "id": "cxujZ8ru8TaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc9e6982-459b-4a6c-8131-7d4ad7bce3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/pandas/utils.py:1017: PandasAPIOnSparkAdviceWarning: The config 'spark.sql.ansi.enabled' is set to True. This can cause unexpected behavior from pandas API on Spark since pandas API on Spark follows the behavior of pandas, not SQL.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/pyspark/pandas/utils.py:1017: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/pyspark/pandas/utils.py:1017: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime = 3.4201674461364746\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "mySchema = [\"medallion\", \"hack_license\", \"pickup_datetime\",\n",
        "            \"dropoff_datetime\", \"trip_time_in_secs\", \"trip_distance\",\n",
        "            \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\n",
        "            \"dropoff_latitude\", \"payment_type\", \"fare_amount\",\n",
        "            \"surcharge\", \"mta_tax\", \"tip_amount\",\n",
        "            \"tolls_amount\", \"total_amount\"]\n",
        "\n",
        "dataset = ps.read_csv(FILENAME,names=mySchema)\n",
        "result = dataset[[\"hack_license\",\"total_amount\"]].groupby(\"hack_license\").sum()\n",
        "# force execution\n",
        "result0 = result.to_spark().collect()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print( \"Runtime = \" + str(end_time - start_time))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "d81bf814-b05b-4e88-9d6b-b31cbff8a580",
          "showTitle": false,
          "title": ""
        },
        "id": "B55JX4hE8TaN"
      },
      "source": [
        "### Results (Spark Pandas API)\n",
        "\n",
        "The time to process the small dataset was : **20.158647775650024** seconds.\n",
        "\n",
        "The time to process the large dataset was : **TO COMPLETE** seconds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "9e667272-66f4-4789-9136-69fe489293b8",
          "showTitle": false,
          "title": ""
        },
        "id": "pPo82rOWXPBg"
      },
      "source": [
        "### Spark SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "aa3a555d-41ce-4301-bb60-9c13bacb2b0c",
          "showTitle": false,
          "title": ""
        },
        "id": "l2On5zrlXPBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18c782f-dc3c-4039-dd03-37a5a0e61ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime = 2.288220167160034\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Group project\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "start_time = time.time()\n",
        "mySchema = StructType([\n",
        "    StructField(\"medallion\", StringType()),\n",
        "    StructField(\"hack_license\", StringType()),\n",
        "    StructField(\"pickup_datetime\", TimestampType()),\n",
        "    StructField(\"dropoff_datetime\", TimestampType()),\n",
        "    StructField(\"trip_time_in_secs\", IntegerType()),\n",
        "    StructField(\"trip_distance\", DoubleType()),\n",
        "    StructField(\"pickup_longitude\", DoubleType()),\n",
        "    StructField(\"pickup_latitude\", DoubleType()),\n",
        "    StructField(\"dropoff_longitude\", DoubleType()),\n",
        "    StructField(\"dropoff_latitude\", DoubleType()),\n",
        "    StructField(\"payment_type\", StringType()),\n",
        "    StructField(\"fare_amount\", DoubleType()),\n",
        "    StructField(\"surcharge\", DoubleType()),\n",
        "    StructField(\"mta_tax\", DoubleType()),\n",
        "    StructField(\"tip_amount\", DoubleType()),\n",
        "    StructField(\"tolls_amount\", DoubleType()),\n",
        "    StructField(\"total_amount\", DoubleType()),\n",
        "])\n",
        "\n",
        "dataset = spark.read.load(FILENAME, format=\"csv\",\n",
        "                         sep=\",\", schema=mySchema, header=\"false\")\n",
        "dataset.createOrReplaceTempView(\"data\")\n",
        "statisticsDF = spark.sql( \"\"\"SELECT hack_license, SUM(total_amount) AS total_amount FROM data GROUP BY hack_license\"\"\")\n",
        "statistics = statisticsDF.collect()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print( \"Runtime = \" + str(end_time - start_time))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "cea0ff85-3d10-4a6a-b24a-4cc66b780f3c",
          "showTitle": false,
          "title": ""
        },
        "id": "Jkrh7oezXPBg"
      },
      "source": [
        "### Results (Spark)\n",
        "\n",
        "The time to process the small dataset was : **6.135289669036865** seconds.\n",
        "\n",
        "The time to process the large dataset was : **TO COMPLETE** seconds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "fbceb57e-91d7-430a-8406-ad44dec1b38a",
          "showTitle": false,
          "title": ""
        },
        "id": "uie2IAwg9PUy"
      },
      "source": [
        "### Code: cuDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "2708ac4e-177f-4c8b-a7c3-b9ebe08a55f2",
          "showTitle": false,
          "title": ""
        },
        "id": "jdXwudaU9PUz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "outputId": "b9d17a34-432f-403a-a577-d7e8cc4a7020"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MemoryError",
          "evalue": "std::bad_alloc: CUDA error at: /pyenv/versions/3.12.9/lib/python3.12/site-packages/librmm/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorInsufficientDriver CUDA driver version is insufficient for CUDA runtime version",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ddbdcf7c6ee8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \"tolls_amount\", \"total_amount\"]\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILENAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hack_license\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"total_amount\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hack_license\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/performance_tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     )\n\u001b[1;32m     50\u001b[0m                 )\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/io/csv.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, prefix, mangle_dupe_cols, dtype, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, dayfirst, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, comment, delim_whitespace, byte_range, storage_options, bytes_per_thread)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_na_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mna_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mtable_w_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m     data = {\n\u001b[1;32m    259\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pylibcudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mcsv.pyx\u001b[0m in \u001b[0;36mpylibcudf.io.csv.read_csv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcsv.pyx\u001b[0m in \u001b[0;36mpylibcudf.io.csv.read_csv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: CUDA error at: /pyenv/versions/3.12.9/lib/python3.12/site-packages/librmm/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorInsufficientDriver CUDA driver version is insufficient for CUDA runtime version"
          ]
        }
      ],
      "source": [
        "####\n",
        "#start_time = time.time()\n",
        "#mySchema = [\"medallion\", \"hack_license\", \"pickup_datetime\",\n",
        "#            \"dropoff_datetime\", \"trip_time_in_secs\", \"trip_distance\",\n",
        " #           \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\n",
        "  #          \"dropoff_latitude\", \"payment_type\", \"fare_amount\",\n",
        "   #         \"surcharge\", \"mta_tax\", \"tip_amount\",\n",
        "    #        \"tolls_amount\", \"total_amount\"]\n",
        "\n",
        "#dataset = cudf.read_csv(FILENAME,names=mySchema)\n",
        "#result = dataset[[\"hack_license\",\"total_amount\"]].groupby(\"hack_license\").sum()\n",
        "\n",
        "#end_time = time.time()\n",
        "\n",
        "#print( \"Runtime = \" + str(end_time - start_time))\n",
        "####"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "mySchema = [\"medallion\", \"hack_license\", \"pickup_datetime\",\n",
        "            \"dropoff_datetime\", \"trip_time_in_secs\", \"trip_distance\",\n",
        "            \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\n",
        "            \"dropoff_latitude\", \"payment_type\", \"fare_amount\",\n",
        "            \"surcharge\", \"mta_tax\", \"tip_amount\",\n",
        "            \"tolls_amount\", \"total_amount\"]\n",
        "\n",
        "# Read the CSV file using pandas with the appropriate encoding if needed\n",
        "dataset_pd = pd.read_csv(FILENAME, names=mySchema, delimiter=\",\", quotechar='\"', encoding='utf-8')\n",
        "\n",
        "# Convert the pandas DataFrame to a cuDF DataFrame\n",
        "dataset = cudf.from_pandas(dataset_pd)\n",
        "\n",
        "# Perform the groupby and sum operation\n",
        "result = dataset[[\"hack_license\",\"total_amount\"]].groupby(\"hack_license\").sum()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print( \"Runtime = \" + str(end_time - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "67zEt4AKRZya",
        "outputId": "0e52760e-bc78-4d1f-df41-6431d456a52e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Fatal CUDA error encountered at: /__w/cudf/cudf/cpp/src/bitmask/null_mask.cu:93: 35 cudaErrorInsufficientDriver CUDA driver version is insufficient for CUDA runtime version",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-be7dd319f1a5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Convert the pandas DataFrame to a cuDF DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_pd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Perform the groupby and sum operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/performance_tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     )\n\u001b[1;32m     50\u001b[0m                 )\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/dataframe.py\u001b[0m in \u001b[0;36mfrom_pandas\u001b[0;34m(obj, nan_as_null)\u001b[0m\n\u001b[1;32m   8425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan_as_null\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnan_as_null\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8428\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8429\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan_as_null\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnan_as_null\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/performance_tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     )\n\u001b[1;32m     50\u001b[0m                 )\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/dataframe.py\u001b[0m in \u001b[0;36mfrom_pandas\u001b[0;34m(cls, dataframe, nan_as_null)\u001b[0m\n\u001b[1;32m   5549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5551\u001b[0;31m             data = {\n\u001b[0m\u001b[1;32m   5552\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan_as_null\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnan_as_null\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5553\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/dataframe.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   5550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5551\u001b[0m             data = {\n\u001b[0;32m-> 5552\u001b[0;31m                 \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnan_as_null\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnan_as_null\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5553\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5554\u001b[0m             }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/column/column.py\u001b[0m in \u001b[0;36mas_column\u001b[0;34m(arbitrary, nan_as_null, dtype, length)\u001b[0m\n\u001b[1;32m   2232\u001b[0m                 \u001b[0mfrom_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2233\u001b[0m             )\n\u001b[0;32m-> 2234\u001b[0;31m             return as_column(\n\u001b[0m\u001b[1;32m   2235\u001b[0m                 \u001b[0mpyarrow_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/column/column.py\u001b[0m in \u001b[0;36mas_column\u001b[0;34m(arbitrary, nan_as_null, dtype, length)\u001b[0m\n\u001b[1;32m   2099\u001b[0m             \u001b[0;31m# default \"empty\" type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"str\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColumnBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marbitrary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/column/column.py\u001b[0m in \u001b[0;36mfrom_arrow\u001b[0;34m(cls, array)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             result = cls.from_pylibcudf(\n\u001b[0;32m--> 397\u001b[0;31m                 \u001b[0mplc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m             )\n\u001b[1;32m    399\u001b[0m             \u001b[0;31m# TODO: cudf_dtype_from_pa_type may be less necessary for some types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    907\u001b[0m                             '1 positional argument')\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32minterop.pyx\u001b[0m in \u001b[0;36mpylibcudf.interop._from_arrow_table\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Fatal CUDA error encountered at: /__w/cudf/cudf/cpp/src/bitmask/null_mask.cu:93: 35 cudaErrorInsufficientDriver CUDA driver version is insufficient for CUDA runtime version"
          ]
        }
      ]
    },
    {
      "source": [
        "!nvidia-smi # for GPU information\n",
        "!nvcc --version # for CUDA version"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "VoXNNgcoJbP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "d81bf814-b05b-4e88-9d6b-b31cbff8a580",
          "showTitle": false,
          "title": ""
        },
        "id": "lRDO1urZ9PUz"
      },
      "source": [
        "### Results (cuDF)\n",
        "\n",
        "The time to process the small dataset was : **2.8160507678985596** seconds.\n",
        "\n",
        "The time to process the large dataset was : **TO COMPLETE** seconds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "ae84b317-68ee-4eec-96c7-f010f52b2757",
          "showTitle": false,
          "title": ""
        },
        "id": "OAe85kD0XPBg"
      },
      "source": [
        "### Results discussion\n",
        "\n",
        "**TO BE COMPLETED**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "The code presented in the **Plotting heatmap** section includes the call to an registered Python UDF function. In this exercise we want to measure the impact of using the UDF function.\n",
        "\n",
        "Run this exercise with the small dataset."
      ],
      "metadata": {
        "id": "WsXbm8ZX9vv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# Squares of 150 meters\n",
        "latitudeStep = 0.0013474668\n",
        "longitudeStep = 0.0017958\n",
        "northLatitude = 40.95\n",
        "southLatitude = northLatitude - 300 * latitudeStep\n",
        "eastLongitude = -74.2\n",
        "westLongitude = eastLongitude + 300 * longitudeStep\n",
        "\n",
        "# function to round longitude to a point in the middle of the square\n",
        "def longiRound( val):\n",
        "    return ((val - eastLongitude) // longitudeStep) * longitudeStep + eastLongitude + longitudeStep / 2\n",
        "spark.udf.register(\"longround\", longiRound, DoubleType())\n",
        "\n",
        "# function to round latitude to a point in the middle of the square\n",
        "def latRound( l):\n",
        "    return northLatitude - ((northLatitude - l) // latitudeStep) * latitudeStep - latitudeStep / 2\n",
        "spark.udf.register(\"latround\", latRound, DoubleType())\n",
        "\n",
        "mySchema = StructType([\n",
        "    StructField(\"medallion\", StringType()),\n",
        "    StructField(\"hack_license\", StringType()),\n",
        "    StructField(\"pickup_datetime\", TimestampType()),\n",
        "    StructField(\"dropoff_datetime\", TimestampType()),\n",
        "    StructField(\"trip_time_in_secs\", IntegerType()),\n",
        "    StructField(\"trip_distance\", DoubleType()),\n",
        "    StructField(\"pickup_longitude\", DoubleType()),\n",
        "    StructField(\"pickup_latitude\", DoubleType()),\n",
        "    StructField(\"dropoff_longitude\", DoubleType()),\n",
        "    StructField(\"dropoff_latitude\", DoubleType()),\n",
        "    StructField(\"payment_type\", StringType()),\n",
        "    StructField(\"fare_amount\", DoubleType()),\n",
        "    StructField(\"surcharge\", DoubleType()),\n",
        "    StructField(\"mta_tax\", DoubleType()),\n",
        "    StructField(\"tip_amount\", DoubleType()),\n",
        "    StructField(\"tolls_amount\", DoubleType()),\n",
        "    StructField(\"total_amount\", DoubleType()),\n",
        "])\n",
        "\n",
        "dataset = spark.read.load(FILENAME, format=\"csv\",\n",
        "                         sep=\",\", schema=mySchema, header=\"false\")\n",
        "\n",
        "# Let's filter data outside of the box and build a grid\n",
        "# Points in each square are mapped to the center of the square.\n",
        "dataset.createOrReplaceTempView(\"data\")\n",
        "filteredDataDF = spark.sql( \"\"\"SELECT medallion, hack_license, pickup_datetime,\n",
        "                                    dropoff_datetime, trip_time_in_secs, trip_distance,\n",
        "                                    longround(pickup_longitude) AS pickup_longitude,\n",
        "                                    latround(pickup_latitude) AS pickup_latitude,\n",
        "                                    longround(dropoff_longitude) AS dropoff_longitude,\n",
        "                                    latround(dropoff_latitude) AS dropoff_latitude,\n",
        "                                    payment_type, fare_amount, mta_tax,\n",
        "                                    tip_amount, tolls_amount, total_amount\n",
        "                                  FROM data\n",
        "                                  WHERE pickup_longitude >= \"\"\" + str(eastLongitude) + \"\"\" AND\n",
        "                                  pickup_longitude <=  \"\"\" + str(westLongitude) + \"\"\" AND\n",
        "                                  dropoff_longitude >=  \"\"\" + str(eastLongitude) + \"\"\" AND\n",
        "                                  dropoff_longitude <=  \"\"\" + str(westLongitude) + \"\"\" AND\n",
        "                                  pickup_latitude <= \"\"\" + str(northLatitude) + \"\"\" AND\n",
        "                                  pickup_latitude >= \"\"\" + str(southLatitude) + \"\"\" AND\n",
        "                                  dropoff_latitude <=  \"\"\" + str(northLatitude) + \"\"\" AND\n",
        "                                  dropoff_latitude >=  \"\"\" + str(southLatitude))\n",
        "filteredDataDF.createOrReplaceTempView(\"data\")\n",
        "\n",
        "# Frequency for pickups\n",
        "pickupsDF = spark.sql( \"\"\"SELECT pickup_longitude, pickup_latitude, count(*) AS cnt\n",
        "                                  FROM data\n",
        "                                  GROUP BY pickup_longitude, pickup_latitude\"\"\")\n",
        "pickups = pickupsDF.collect()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print( \"Runtime = \" + str(end_time - start_time))\n"
      ],
      "metadata": {
        "id": "NM8Zflw09vJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results (Spark SQL with UDF Python function)**\n",
        "\n",
        "The time to process the small dataset was : **13.147183656692505** seconds."
      ],
      "metadata": {
        "id": "j98TJynFB_vH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the code in Spark SQL without using the auxiliary function."
      ],
      "metadata": {
        "id": "nhP0Ndte_84h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# THis part gonna stay the same\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Squares of 150 meters\n",
        "latitudeStep = 0.0013474668\n",
        "longitudeStep = 0.0017958\n",
        "northLatitude = 40.95\n",
        "southLatitude = northLatitude - 300 * latitudeStep\n",
        "eastLongitude = -74.2\n",
        "westLongitude = eastLongitude + 300 * longitudeStep\n",
        "\n",
        "mySchema = StructType([\n",
        "    StructField(\"medallion\", StringType()),\n",
        "    StructField(\"hack_license\", StringType()),\n",
        "    StructField(\"pickup_datetime\", TimestampType()),\n",
        "    StructField(\"dropoff_datetime\", TimestampType()),\n",
        "    StructField(\"trip_time_in_secs\", IntegerType()),\n",
        "    StructField(\"trip_distance\", DoubleType()),\n",
        "    StructField(\"pickup_longitude\", DoubleType()),\n",
        "    StructField(\"pickup_latitude\", DoubleType()),\n",
        "    StructField(\"dropoff_longitude\", DoubleType()),\n",
        "    StructField(\"dropoff_latitude\", DoubleType()),\n",
        "    StructField(\"payment_type\", StringType()),\n",
        "    StructField(\"fare_amount\", DoubleType()),\n",
        "    StructField(\"surcharge\", DoubleType()),\n",
        "    StructField(\"mta_tax\", DoubleType()),\n",
        "    StructField(\"tip_amount\", DoubleType()),\n",
        "    StructField(\"tolls_amount\", DoubleType()),\n",
        "    StructField(\"total_amount\", DoubleType()),\n",
        "])\n",
        "\n",
        "dataset = spark.read.load(FILENAME, format=\"csv\",\n",
        "                         sep=\",\", schema=mySchema, header=\"false\")\n",
        "\n",
        "\n",
        "# Intergrated Version instead of def long/lat we included into the select statement of our SQL\n",
        "dataset.createOrReplaceTempView(\"data\")\n",
        "filteredDataDF = spark.sql(f\"\"\"\n",
        "SELECT\n",
        "    medallion, hack_license, pickup_datetime,\n",
        "    dropoff_datetime, trip_time_in_secs, trip_distance,\n",
        "\n",
        "    -- Round pickup_longitude\n",
        "    (FLOOR((pickup_longitude - {eastLongitude}) / {longitudeStep}) * {longitudeStep} +\n",
        "     {eastLongitude} + {longitudeStep} / 2) AS pickup_longitude,\n",
        "\n",
        "    -- Round pickup_latitude\n",
        "    ({northLatitude} - FLOOR(({northLatitude} - pickup_latitude) / {latitudeStep}) * {latitudeStep} -\n",
        "     {latitudeStep} / 2) AS pickup_latitude,\n",
        "\n",
        "    -- Round dropoff_longitude\n",
        "    (FLOOR((dropoff_longitude - {eastLongitude}) / {longitudeStep}) * {longitudeStep} +\n",
        "     {eastLongitude} + {longitudeStep} / 2) AS dropoff_longitude,\n",
        "\n",
        "    -- Round dropoff_latitude\n",
        "    ({northLatitude} - FLOOR(({northLatitude} - dropoff_latitude) / {latitudeStep}) * {latitudeStep} -\n",
        "     {latitudeStep} / 2) AS dropoff_latitude,\n",
        "\n",
        "    payment_type, fare_amount, mta_tax, tip_amount, tolls_amount, total_amount\n",
        "FROM data\n",
        "WHERE pickup_longitude BETWEEN {eastLongitude} AND {westLongitude}\n",
        "  AND dropoff_longitude BETWEEN {eastLongitude} AND {westLongitude}\n",
        "  AND pickup_latitude BETWEEN {southLatitude} AND {northLatitude}\n",
        "  AND dropoff_latitude BETWEEN {southLatitude} AND {northLatitude}\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "filteredDataDF.createOrReplaceTempView(\"data\")\n",
        "\n",
        "# Frequency for pickups\n",
        "pickupsDF = spark.sql( \"\"\"SELECT pickup_longitude, pickup_latitude, count(*) AS cnt\n",
        "                                  FROM data\n",
        "                                  GROUP BY pickup_longitude, pickup_latitude\"\"\")\n",
        "pickups = pickupsDF.collect()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print( \"Runtime = \" + str(end_time - start_time))\n",
        "\n"
      ],
      "metadata": {
        "id": "BqxPIworAHnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results (Spark SQL with no UDF Python function)**\n",
        "\n",
        "The time to process the small dataset was : **9.010905504226685** seconds.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8-N9UcmHCKGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion of results\n",
        "\n",
        "Replacing Python UDFs with native SQL expressions results in better performance, more efficient resource usage, and faster query execution — especially when working with large-scale geospatial data like taxi trip records.\n",
        "\n",
        "*UDFs recommended:*\n",
        "\n",
        "•⁠  ⁠When the logic is too complex to be expressed directly in SQL.\n",
        "\n",
        "•⁠  ⁠When prioritizing code readability, modularity, or reusability, even if it comes with some performance trade-offs.\n",
        "\n",
        "•⁠  ⁠When working with data types or formats that are not natively supported by Spark SQL."
      ],
      "metadata": {
        "id": "Opyeux-SEsRq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "b5345a02-01de-46da-aaeb-150747d13ae4",
          "showTitle": false,
          "title": ""
        },
        "id": "Wi23Oz_WXPBg"
      },
      "source": [
        "## Exercise 3\n",
        "\n",
        "Let's start by trying to help the city to identify which new express bus routes shoud introduce. To this end, you should find the 20 most frequent routes whose distance is above a given treshold (defined by you).\n",
        "\n",
        "For establishing these routes, we suggest that you use a grid of 500m of side.\n",
        "\n",
        "Write two solutions: one using Spark SQL and the other Spark Pandas API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "33b92f6b-d79e-439b-a73c-3b7c28062d58",
          "showTitle": false,
          "title": ""
        },
        "id": "4SPBmJfBXPBg"
      },
      "outputs": [],
      "source": [
        "# COMPLETE with Spark SQL code\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Squares of ~500 meters\n",
        "\n",
        "latitudeStep = 0.004491556\n",
        "longitudeStep = 0.005986\n",
        "northLatitude = 40.95\n",
        "southLatitude = northLatitude - 300 * latitudeStep\n",
        "eastLongitude = -74.2\n",
        "westLongitude = eastLongitude + 300 * longitudeStep"
      ],
      "metadata": {
        "id": "Bd0tQDIC7xt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "routesDF = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    pickup_longitude,\n",
        "    pickup_latitude,\n",
        "    dropoff_longitude,\n",
        "    dropoff_latitude,\n",
        "    COUNT(*) AS cnt,\n",
        "    AVG(trip_distance) AS avg_trip_distance_miles\n",
        "FROM data\n",
        "GROUP BY\n",
        "    pickup_longitude,\n",
        "    pickup_latitude,\n",
        "    dropoff_longitude,\n",
        "    dropoff_latitude\n",
        "ORDER BY cnt DESC\n",
        "LIMIT 100\n",
        "\"\"\")\n",
        "routesDF.show(20)"
      ],
      "metadata": {
        "id": "VEZGOvQT71cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average Longest Trip Distances\n",
        "\n",
        "routesDF = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    pickup_longitude,\n",
        "    pickup_latitude,\n",
        "    dropoff_longitude,\n",
        "    dropoff_latitude,\n",
        "    COUNT(*) AS cnt,\n",
        "    AVG(trip_distance) AS avg_trip_distance_miles\n",
        "FROM data\n",
        "GROUP BY\n",
        "    pickup_longitude,\n",
        "    pickup_latitude,\n",
        "    dropoff_longitude,\n",
        "    dropoff_latitude\n",
        "ORDER BY avg_trip_distance_miles DESC, cnt DESC\n",
        "LIMIT 100\n",
        "\"\"\")\n",
        "routesDF.show(20)"
      ],
      "metadata": {
        "id": "tmvQVsRW73xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting via Distandce and count\n",
        "routesDF.orderBy([\"avg_trip_distance_miles\", \"cnt\"], ascending=[False, False]).show(50)"
      ],
      "metadata": {
        "id": "dzidEzmO75z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defininng a treshhold based on avg trip distance 10 miles or bigger\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "routesDF = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    pickup_longitude,\n",
        "    pickup_latitude,\n",
        "    dropoff_longitude,\n",
        "    dropoff_latitude,\n",
        "    COUNT(*) AS cnt,\n",
        "    AVG(trip_distance) AS avg_trip_distance_miles\n",
        "FROM data\n",
        "GROUP BY\n",
        "    pickup_longitude,\n",
        "    pickup_latitude,\n",
        "    dropoff_longitude,\n",
        "    dropoff_latitude\n",
        "HAVING avg_trip_distance_miles >= 15\n",
        "ORDER BY cnt DESC\n",
        "LIMIT 100\n",
        "\"\"\")\n",
        "\n",
        "# Show the top results\n",
        "routesDF.show(20)\n",
        "\n",
        "# Stop the timer after query execution\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the runtime\n",
        "runtime = end_time - start_time\n",
        "print(f\"SQL query execution time: {runtime} seconds\")"
      ],
      "metadata": {
        "id": "zRbtN3ee7-iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "routesDF = spark.sql(\"\"\"\n",
        "SELECT\n",
        "    pickup_longitude,\n",
        "    pickup_latitude,\n",
        "    dropoff_longitude,\n",
        "    dropoff_latitude,\n",
        "    COUNT(*) AS cnt,\n",
        "    AVG(trip_distance) AS avg_trip_distance_miles\n",
        "FROM data\n",
        "GROUP BY\n",
        "    pickup_longitude,\n",
        "    pickup_latitude,\n",
        "    dropoff_longitude,\n",
        "    dropoff_latitude\n",
        "HAVING avg_trip_distance_miles >= 15\n",
        "ORDER BY cnt DESC\n",
        "LIMIT 100\n",
        "\"\"\")\n",
        "\n",
        "# Stop the timer after query execution\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the runtime\n",
        "runtime = end_time - start_time\n",
        "print(f\"SQL query execution time: {runtime} seconds\")"
      ],
      "metadata": {
        "id": "XLFfK_aJ8Bgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE with Spark Pandas API code\n",
        "\n",
        "# Start performance timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Load the dataset using Spark's Pandas API (assumes CSV format with header)\n",
        "psdf = ps.read_csv(FILENAME, header=None, names=[\n",
        "    \"medallion\", \"hack_license\", \"pickup_datetime\", \"dropoff_datetime\",\n",
        "    \"trip_time_in_secs\", \"trip_distance\", \"pickup_longitude\", \"pickup_latitude\",\n",
        "    \"dropoff_longitude\", \"dropoff_latitude\", \"payment_type\", \"fare_amount\",\n",
        "    \"surcharge\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"total_amount\"\n",
        "])\n",
        "\n",
        "# Filter rows where trip distance > 15 miles\n",
        "filtered_psdf = psdf[psdf[\"trip_distance\"] > 15]\n",
        "\n",
        "# Group by pickup and dropoff coordinates\n",
        "grouped = filtered_psdf.groupby([\n",
        "    \"pickup_longitude\", \"pickup_latitude\",\n",
        "    \"dropoff_longitude\", \"dropoff_latitude\"\n",
        "])\n",
        "\n",
        "# Aggregate to get count and average trip distance\n",
        "routes_summary = grouped.agg(\n",
        "    cnt=(\"trip_distance\", \"count\"),\n",
        "    avg_trip_distance_miles=(\"trip_distance\", \"mean\")\n",
        ").reset_index()\n",
        "\n",
        "# Sort by average distance and count (both descending)\n",
        "sorted_routes = routes_summary.sort_values(\n",
        "    by=[\"avg_trip_distance_miles\", \"cnt\"],\n",
        "    ascending=[False, False]\n",
        ")\n",
        "\n",
        "# Show top 20 routes\n",
        "print(sorted_routes.head(20))\n",
        "\n",
        "# Stop performance timer\n",
        "end_time = time.time()\n",
        "print(f\"Execution time (Spark Pandas API): {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "SXNJQdAXHFaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "303faa61-b5b1-4ac9-a08e-905b9e4c5708",
          "showTitle": false,
          "title": ""
        },
        "id": "qenXFMnDXPBg"
      },
      "source": [
        "#### Discussion\n",
        "\n",
        "**COMPLETE: Explain your code and discuss which one is preferable from your point of view**\n",
        "\n",
        "We adjusted the squares as suggested in the exercise and then analyzed the most frequent routes, incorporating the average trip distance. With the small dataset, most trip distances were found to be between 1 and 3 miles. We then set a threshold of 15 miles and above for the average trip distances and recalculated the top 20 routes.\n",
        "\n",
        "When comparing Spark SQL and Spark Pandas API, using the small dataset, we observed that SQL executed significantly faster than Spark Pandas API. While, we are more familiar with Pandas we would still choose SQL for better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "85a2968b-96fa-4772-8e58-f6cdbe580f77",
          "showTitle": false,
          "title": ""
        },
        "id": "GEKUGl1LXPBg"
      },
      "source": [
        "## Exercise 4\n",
        "\n",
        "This question intends to define the location of taxi ranks / taxi stand (the places where taxis stop waiting for collecting clients) in a way that tries to minimize the distance a client needs to walk to reach a taxi rank.\n",
        "\n",
        "Run this exercise with the small dataset and compare the execution time when using scikit-learn, cuML and Spark MLib.\n",
        "\n",
        "**Note:** This dataset is for NYC taxis. So, pickups outside of the city are infrequent and not representative of the demand in such areas. As such, you should focus on pickups in a square that includes NYC (it is ok if the square includes parts outside of the city). Use, for example, the following square:\n",
        "```\n",
        "northLatitude = 40.86\n",
        "southLatitude = 40.68\n",
        "eastLongitude = -74.03\n",
        "westLongitude = -73.92\n",
        "```\n",
        "\n",
        "**Suggestion:** Plot your results as a heatmap, with the color being a measurement of the value of the taxi rank."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find important taxi stands in NYC we using a clustering analysis.\n",
        "- Map of NYC with the square (in red)\n",
        "- Elbow (based on the tiny dataset)\n",
        "- Cluster Analysis\n",
        "- Performance Comparison"
      ],
      "metadata": {
        "id": "OVpmPdfG4aPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "nxGyGBuFwHaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install geopandas shapely contextily matplotlib"
      ],
      "metadata": {
        "id": "HuhYZ1htBOXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, box\n",
        "import matplotlib.pyplot as plt\n",
        "import contextily as ctx\n",
        "\n",
        "# Step 1: Define your target box in EPSG:4326 (lat/lon)\n",
        "northLatitude = 40.86\n",
        "southLatitude = 40.68\n",
        "eastLongitude = -74.03\n",
        "westLongitude = -73.92\n",
        "\n",
        "highlight_box = box(eastLongitude, southLatitude, westLongitude, northLatitude)\n",
        "highlight_gdf = gpd.GeoDataFrame({'geometry': [highlight_box]}, crs=\"EPSG:4326\").to_crs(epsg=3857)\n",
        "\n",
        "# Convert pickup data to GeoDataFrame, including 'cnt' column\n",
        "pickup_points = gpd.GeoDataFrame(\n",
        "    pickups,\n",
        "    geometry=[Point(row.pickup_longitude, row.pickup_latitude) for row in pickups],\n",
        "    crs=\"EPSG:4326\"  # WGS84\n",
        ").to_crs(epsg=3857)  # Project to match NYC map/grid\n",
        "\n",
        "pickup_points['cnt'] = [row.cnt for row in pickups]\n",
        "\n",
        "# Same for dropoffs\n",
        "dropoff_points = gpd.GeoDataFrame(\n",
        "    dropoffs,\n",
        "    geometry=[Point(row.dropoff_longitude, row.dropoff_latitude) for row in dropoffs],\n",
        "    crs=\"EPSG:4326\"\n",
        ").to_crs(epsg=3857)\n",
        "\n",
        "dropoff_points['cnt'] = [row.cnt for row in dropoffs]\n",
        "\n",
        "# Step 2: Plot full map + grid + highlighted box\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "# Plot NYC boundary and grid\n",
        "nyc.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=1)\n",
        "grid.plot(ax=ax, facecolor='none', edgecolor='gray', linewidth=0.5)\n",
        "\n",
        "# Step 3: Plot pickups with intensity based on frequency\n",
        "pickup_points.plot(\n",
        "    ax=ax,\n",
        "    column='cnt',  # Color by count\n",
        "    cmap='rainbow',\n",
        "    markersize=5,\n",
        "    alpha=0.6,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "# Step 4: Optionally, plot dropoffs similarly (if needed)\n",
        "# dropoff_points.plot(ax=ax, column='cnt', cmap='cool', markersize=2, alpha=0.5)\n",
        "\n",
        "# Step 5: Highlight the defined box region\n",
        "highlight_gdf.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=3, linestyle='--')\n",
        "\n",
        "# Add basemap and styling\n",
        "ctx.add_basemap(ax, crs=nyc.crs, source=ctx.providers.CartoDB.Positron)\n",
        "\n",
        "# Title\n",
        "ax.set_title(\"NYC Pickup and Dropoff Density with Highlighted Region\")\n",
        "\n",
        "# Turn off axis for better map presentation\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PIP8ZOs0wGha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "I7MxieErLLpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ELbow Technique to determine number of clusters only execute on small df\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inertias_sklearn = []\n",
        "K_range = range(1, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    model = KMeans(n_clusters=k, random_state=42)\n",
        "    model.fit(df)\n",
        "    inertias_sklearn.append(model.inertia_)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K_range, inertias_sklearn, 'o-', label='scikit-learn')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Inertia (Within-cluster SSE)')\n",
        "plt.title('Elbow Method for Optimal k (scikit-learn)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ce5Tvx7R4Tf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimal Cluster number 3**"
      ],
      "metadata": {
        "id": "0NIEQd095KnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for NYC bounds\n",
        "northLatitude = 40.86\n",
        "southLatitude = 40.68\n",
        "eastLongitude = -74.03\n",
        "westLongitude = -73.92\n",
        "\n",
        "df_filtered = df[\n",
        "    (df['pickup_latitude'] >= southLatitude) & (df['pickup_latitude'] <= northLatitude) &\n",
        "    (df['pickup_longitude'] >= westLongitude) & (df['pickup_longitude'] <= eastLongitude)\n",
        "][['pickup_longitude', 'pickup_latitude']]\n"
      ],
      "metadata": {
        "id": "9Dq4p0fixhrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE with code using scikit-learn"
      ],
      "metadata": {
        "id": "pis2MPcs25k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import time\n",
        "\n",
        "# Start timer\n",
        "start = time.time()\n",
        "\n",
        "# Fit KMeans to pickup points within the NYC bounds and grid\n",
        "kmeans_sklearn = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_sklearn.fit(df_filtered)  # Fit to the filtered DataFrame\n",
        "\n",
        "# End timer\n",
        "end = time.time()\n",
        "print(f\"scikit-learn KMeans execution time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Get centroids\n",
        "centroids_sklearn = kmeans_sklearn.cluster_centers_"
      ],
      "metadata": {
        "id": "SD6jvY93xjST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE with code using cuML\n"
      ],
      "metadata": {
        "id": "MJHH-Abd28uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from cuml.cluster import KMeans as cuKMeans\n",
        "import cupy as cp\n",
        "import time\n",
        "import cudf\n",
        "\n",
        "# 1. Data Loading with Timing\n",
        "start_load = time.time()\n",
        "df = cudf.read_csv(FILENAME)  # Direct cuDF loading\n",
        "end_load = time.time()\n",
        "print(f\"Data loading time (cuDF): {end_load - start_load:.2f} seconds\")\n",
        "\n",
        "# 2. Define NYC bounding box\n",
        "northLatitude = 40.86\n",
        "southLatitude = 40.68\n",
        "eastLongitude = -74.03\n",
        "westLongitude = -73.92\n",
        "\n",
        "# 3. GPU-Based Filtering with Timing\n",
        "start_filter = time.time()\n",
        "df_filtered = df[\n",
        "    (df['pickup_latitude'] >= southLatitude) & (df['pickup_latitude'] <= northLatitude) &\n",
        "    (df['pickup_longitude'] >= westLongitude) & (df['pickup_longitude'] <= eastLongitude)\n",
        "]\n",
        "end_filter = time.time()\n",
        "print(f\"Filtering time (cuDF): {end_filter - start_filter:.2f} seconds\")\n",
        "\n",
        "# 4. Convert to cupy array\n",
        "X_cuml = df_filtered[['pickup_longitude', 'pickup_latitude']].values\n",
        "\n",
        "# 5. KMeans Clustering with Timing\n",
        "start_kmeans = time.time()\n",
        "kmeans_cuml = cuKMeans(n_clusters=3, random_state=42, init='k-means++', max_iter=300, tol=1e-4)\n",
        "kmeans_cuml.fit(X_cuml)\n",
        "end_kmeans = time.time()\n",
        "print(f\"cuML KMeans execution time: {end_kmeans - start_kmeans:.2f} seconds\")\n",
        "\n",
        "# 6. Get centroids\n",
        "centroids_cuml = kmeans_cuml.cluster_centers_\n",
        "\n",
        "# Total execution time (excluding data loading)\n",
        "total_time = (end_filter - start_filter) + (end_kmeans - start_kmeans)\n",
        "print(f\"Total execution time (filtering + KMeans): {total_time:.2f} seconds\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "N1QiZyYHOPji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE with code using Spark Mlib"
      ],
      "metadata": {
        "id": "v41r7da62_hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans as SparkKMeans\n",
        "import time\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"TaxiKMeans\").getOrCreate()\n",
        "\n",
        "# Create Spark DataFrame\n",
        "spark_df = spark.createDataFrame(df)\n",
        "\n",
        "# Assemble features\n",
        "vec_assembler = VectorAssembler(\n",
        "    inputCols=[\"pickup_longitude\", \"pickup_latitude\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "spark_vec_df = vec_assembler.transform(spark_df)\n",
        "\n",
        "# Run KMeans\n",
        "start = time.time()\n",
        "kmeans = SparkKMeans(k=3, seed=42)\n",
        "model = kmeans.fit(spark_vec_df)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Spark MLlib KMeans execution time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Get centroids\n",
        "centroids_spark = model.clusterCenters()\n"
      ],
      "metadata": {
        "id": "NwKeBKkYyRfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans as SparkKMeans\n",
        "import time\n",
        "\n",
        "# ... (Previous code for grid filtering) ...\n",
        "\n",
        "\n",
        "# Start Spark session (if not already started)\n",
        "# spark = SparkSession.builder.appName(\"TaxiKMeans\").getOrCreate()\n",
        "\n",
        "# Create Spark DataFrame from filtered data\n",
        "spark_df_filtered = spark.createDataFrame(df_filtered)  # Use df_filtered\n",
        "\n",
        "# Assemble features\n",
        "vec_assembler = VectorAssembler(\n",
        "    inputCols=[\"pickup_longitude\", \"pickup_latitude\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "spark_vec_df = vec_assembler.transform(spark_df_filtered)  # Use filtered data\n",
        "\n",
        "# Run KMeans\n",
        "start = time.time()\n",
        "kmeans = SparkKMeans(k=3, seed=42)  # k=3 for consistency\n",
        "model = kmeans.fit(spark_vec_df)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Spark MLlib KMeans execution time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Get centroids\n",
        "centroids_spark = model.clusterCenters()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "J04_4qghMI-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans as SparkKMeans\n",
        "import time\n",
        "\n",
        "# ... (Previous code to define grid, UDFs, etc.) ...\n",
        "\n",
        "# Define NYC bounding box\n",
        "northLatitude = 40.86\n",
        "southLatitude = 40.68\n",
        "eastLongitude = -74.03\n",
        "westLongitude = -73.92\n",
        "\n",
        "# Start Spark session (if not already started)\n",
        "# spark = SparkSession.builder.appName(\"TaxiKMeans\").getOrCreate()\n",
        "\n",
        "# Filter the Spark DataFrame directly\n",
        "spark_df_filtered = spark_df.filter(\n",
        "    (col(\"pickup_latitude\") >= southLatitude) & (col(\"pickup_latitude\") <= northLatitude) &\n",
        "    (col(\"pickup_longitude\") >= westLongitude) & (col(\"pickup_longitude\") <= eastLongitude)\n",
        ")\n",
        "\n",
        "# Assemble features\n",
        "vec_assembler = VectorAssembler(\n",
        "    inputCols=[\"pickup_longitude\", \"pickup_latitude\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "spark_vec_df = vec_assembler.transform(spark_df_filtered)\n",
        "\n",
        "# Run KMeans\n",
        "start = time.time()\n",
        "kmeans = SparkKMeans(k=3, seed=42)  # k=3 for consistency\n",
        "model = kmeans.fit(spark_vec_df)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Spark MLlib KMeans execution time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Get centroids\n",
        "centroids_spark = model.clusterCenters()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "wcQ-lb3NMsPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, box\n",
        "import matplotlib.pyplot as plt\n",
        "import contextily as ctx\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Step 1: Define your target box in EPSG:4326 (lat/lon)\n",
        "northLatitude = 40.86\n",
        "southLatitude = 40.68\n",
        "eastLongitude = -74.03\n",
        "westLongitude = -73.92\n",
        "\n",
        "highlight_box = box(eastLongitude, southLatitude, westLongitude, northLatitude)\n",
        "highlight_gdf = gpd.GeoDataFrame({'geometry': [highlight_box]}, crs=\"EPSG:4326\").to_crs(epsg=3857)\n",
        "\n",
        "# Step 2: Check the column names to ensure we reference them correctly\n",
        "print(pickups.columns)  # Check column names in pickups DataFrame\n",
        "print(dropoffs.columns)  # Check column names in dropoffs DataFrame\n",
        "\n",
        "# Step 3: Convert pickup data to GeoDataFrame, including 'cnt' column\n",
        "# Replace 'pickup_longitude' and 'pickup_latitude' with the correct column names.\n",
        "pickup_points = gpd.GeoDataFrame(\n",
        "    pickups,\n",
        "    geometry=[Point(row['pickup_longitude'], row['pickup_latitude']) for idx, row in pickups.iterrows()],\n",
        "    crs=\"EPSG:4326\"  # WGS84\n",
        ").to_crs(epsg=3857)  # Project to match NYC map/grid\n",
        "\n",
        "pickup_points['cnt'] = pickups['cnt']  # Assuming you have a 'cnt' column in pickups DataFrame\n",
        "\n",
        "# Same for dropoffs\n",
        "dropoff_points = gpd.GeoDataFrame(\n",
        "    dropoffs,\n",
        "    geometry=[Point(row['dropoff_longitude'], row['dropoff_latitude']) for idx, row in dropoffs.iterrows()],\n",
        "    crs=\"EPSG:4326\"\n",
        ").to_crs(epsg=3857)\n",
        "\n",
        "dropoff_points['cnt'] = dropoffs['cnt']  # Assuming you have a 'cnt' column in dropoffs DataFrame\n",
        "\n",
        "# Step 4: Run KMeans clustering on pickups (correctly referencing the columns)\n",
        "# You may need to replace 'pickup_longitude' and 'pickup_latitude' with the actual column names from your data.\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "pickup_points['cluster'] = kmeans.fit_predict(pickup_points[['pickup_longitude', 'pickup_latitude']])\n",
        "\n",
        "# Step 5: Plot full map + grid + highlighted box\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "# Plot NYC boundary and grid\n",
        "nyc.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=1)\n",
        "grid.plot(ax=ax, facecolor='none', edgecolor='gray', linewidth=0.5)\n",
        "\n",
        "# Step 6: Plot pickups with intensity based on frequency\n",
        "pickup_points.plot(\n",
        "    ax=ax,\n",
        "    column='cnt',  # Color by count\n",
        "    cmap='rainbow',\n",
        "    markersize=5,\n",
        "    alpha=0.6,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "# Step 7: Plot clusters with different colors\n",
        "cluster_colors = {0: 'red', 1: 'blue', 2: 'green'}\n",
        "for cluster in pickup_points['cluster'].unique():\n",
        "    cluster_points = pickup_points[pickup_points['cluster'] == cluster]\n",
        "    cluster_points.plot(\n",
        "        ax=ax,\n",
        "        color=cluster_colors[cluster],\n",
        "        markersize=10,\n",
        "        label=f\"Cluster {cluster+1}\"\n",
        "    )\n",
        "\n",
        "# Step 8: Plot dropoffs if needed (optional)\n",
        "dropoff_points.plot(ax=ax, column='cnt', cmap='cool', markersize=2, alpha=0.5, legend=True)\n",
        "\n",
        "# Step 9: Highlight the defined box region\n",
        "highlight_gdf.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=3, linestyle='--')\n",
        "\n",
        "# Add basemap and styling\n",
        "ctx.add_basemap(ax, crs=nyc.crs, source=ctx.providers.CartoDB.Positron)\n",
        "\n",
        "# Title\n",
        "ax.set_title(\"NYC Pickup and Dropoff Density with Clusters and Highlighted Region\")\n",
        "\n",
        "# Turn off axis for better map presentation\n",
        "plt.axis('off')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TkmTFGX2AlTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "gHME_zuDwJzV"
      }
    },
    {
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import box\n",
        "import matplotlib.pyplot as plt\n",
        "import contextily as ctx\n",
        "import numpy as np\n",
        "!pip install geodatasets\n",
        "from geodatasets import get_path\n",
        "\n",
        "# Step 1: Load NYC boundary using geodatasets\n",
        "# NYC borough boundaries from a common public dataset\n",
        "nyc = gpd.read_file(get_path(\"nybb\")).to_crs(epsg=3857)  # Web Mercator\n",
        "\n",
        "# Step 2: Create a 500-meter grid over NYC\n",
        "def create_grid(gdf, grid_size):\n",
        "    bounds = gdf.total_bounds  # (minx, miny, maxx, maxy)\n",
        "    xmin, ymin, xmax, ymax = bounds\n",
        "    rows = int(np.ceil((ymax - ymin) / grid_size))\n",
        "    cols = int(np.ceil((xmax - xmin) / grid_size))\n",
        "\n",
        "    grid_cells = []\n",
        "    for i in range(cols):\n",
        "        for j in range(rows):\n",
        "            x0 = xmin + i * grid_size\n",
        "            x1 = x0 + grid_size\n",
        "            y0 = ymin + j * grid_size\n",
        "            y1 = y0 + grid_size\n",
        "            grid_cells.append(box(x0, y0, x1, y1))\n",
        "\n",
        "    grid = gpd.GeoDataFrame({'geometry': grid_cells}, crs=gdf.crs)\n",
        "    return grid\n",
        "\n",
        "grid_size_m = 500  # 500 meters\n",
        "grid = create_grid(nyc, grid_size_m)\n",
        "\n",
        "# Optional: Clip grid to NYC shape\n",
        "grid = gpd.overlay(grid, nyc, how=\"intersection\")\n",
        "\n",
        "# Step 3: Plot everything\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "nyc.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=1)\n",
        "grid.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=0.5)\n",
        "ctx.add_basemap(ax, crs=nyc.crs, source=ctx.providers.CartoDB.Positron)\n",
        "ax.set_title(\"NYC with 500m Grid\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mQkXPFpABtAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert pickup data to GeoDataFrame, including 'cnt' column\n",
        "pickup_points = gpd.GeoDataFrame(\n",
        "    pickups,\n",
        "    geometry=[Point(row.pickup_longitude, row.pickup_latitude) for row in pickups],\n",
        "    crs=\"EPSG:4326\"  # WGS84\n",
        ").to_crs(epsg=3857)  # Project to match NYC map/grid\n",
        "\n",
        "# Explicitly add the 'cnt' column to the GeoDataFrame\n",
        "pickup_points['cnt'] = [row.cnt for row in pickups]\n",
        "\n",
        "\n",
        "# (Optional) Same for dropoffs\n",
        "dropoff_points = gpd.GeoDataFrame(\n",
        "    dropoffs,\n",
        "    geometry=[Point(row.dropoff_longitude, row.dropoff_latitude) for row in dropoffs],\n",
        "    crs=\"EPSG:4326\"\n",
        ").to_crs(epsg=3857)\n",
        "\n",
        "# Add a 'cnt' column for dropoffs as well (if needed)\n",
        "dropoff_points['cnt'] = [row.cnt for row in dropoffs]\n",
        "\n",
        "# Plot on top of NYC map + grid\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "nyc.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=1)\n",
        "grid.plot(ax=ax, facecolor='none', edgecolor='gray', linewidth=0.5)\n",
        "\n",
        "# Plot pickups with intensity based on frequency\n",
        "pickup_points.plot(\n",
        "    ax=ax,\n",
        "    column='cnt',  # Color by count\n",
        "    cmap='rainbow',\n",
        "    markersize=5,\n",
        "    alpha=0.6,\n",
        "    legend=True\n",
        ")\n",
        "\n",
        "# Optional: overlay dropoffs similarly\n",
        "# dropoff_points.plot(ax=ax, column='cnt', cmap='cool', markersize=2, alpha=0.5)\n",
        "\n",
        "ctx.add_basemap(ax, crs=nyc.crs, source=ctx.providers.CartoDB.Positron)\n",
        "ax.set_title(\"NYC Pickup Density with 500m Grid\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BqwkgGz7C9-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from shapely.geometry import box\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import contextily as ctx\n",
        "\n",
        "# Step 1: Define your target box in EPSG:4326 (lat/lon)\n",
        "northLatitude = 40.86\n",
        "southLatitude = 40.68\n",
        "eastLongitude = -74.03\n",
        "westLongitude = -73.92\n",
        "\n",
        "highlight_box = box(eastLongitude, southLatitude, westLongitude, northLatitude)\n",
        "highlight_gdf = gpd.GeoDataFrame({'geometry': [highlight_box]}, crs=\"EPSG:4326\").to_crs(epsg=3857)\n",
        "\n",
        "# Step 2: Plot full map + grid + highlighted box\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "# Full NYC boundary and grid\n",
        "nyc.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=1)\n",
        "grid.plot(ax=ax, facecolor='none', edgecolor='gray', linewidth=0.5)\n",
        "\n",
        "# (Optional) pickup points if available\n",
        "# pickup_points.plot(ax=ax, column='cnt', cmap='rainbow', markersize=2, alpha=0.5)\n",
        "\n",
        "# Step 3: Highlighted box in a bright color\n",
        "highlight_gdf.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=3, linestyle='--')\n",
        "\n",
        "# Add basemap and styling\n",
        "ctx.add_basemap(ax, crs=nyc.crs, source=ctx.providers.CartoDB.Positron)\n",
        "ax.set_title(\"NYC Map with Highlighted Region\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0aytEKtfKMRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load only the pickup longitude and latitude\n",
        "df = pd.read_csv(FILENAME, usecols=[6, 7], names=[\n",
        "    \"pickup_longitude\", \"pickup_latitude\"], skiprows=1)\n",
        "\n",
        "# Define NYC bounding box\n",
        "northLatitude = 40.86\n",
        "southLatitude = 40.68\n",
        "eastLongitude = -74.03\n",
        "westLongitude = -73.92\n",
        "\n",
        "# Filter the data within bounding box\n",
        "df = df.dropna()\n",
        "df = df[\n",
        "    (df[\"pickup_latitude\"].between(southLatitude, northLatitude)) &\n",
        "    (df[\"pickup_longitude\"].between(eastLongitude, westLongitude))\n",
        "]\n",
        "\n",
        "# Sample after filtering (IMPORTANT for performance)\n",
        "#sample_df = df.sample(n=5000, random_state=42)\n",
        "\n",
        "# Extract values for clustering\n",
        "X = df[[\"pickup_longitude\", \"pickup_latitude\"]].values"
      ],
      "metadata": {
        "id": "RiTN6hyLO9Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE with code using scikit-learn\n",
        "# COMPLETE with code using scikit-learn\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "# KMeans Clustering\n",
        "start = time.time()\n",
        "kmeans = KMeans(n_clusters=3, random_state=42).fit(X)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Scikit-learn KMeans Time: {end - start:.2f}s\")\n",
        "\n",
        "# Visualize heatmap (as density or cluster centers)\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.kdeplot(x=X[:, 0], y=X[:, 1], fill=True, cmap=\"Reds\", alpha=0.6, bw_adjust=0.5)\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='blue', marker='X', s=100, label='Taxi Ranks')\n",
        "plt.title(\"Taxi Rank Heatmap (Scikit-learn)\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n6bsHcYuLd0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import contextily as ctx\n",
        "import matplotlib.pyplot as plt\n",
        "from geodatasets import get_path\n",
        "\n",
        "# ── Your square\n",
        "northLatitude = 40.86\n",
        "southLatitude = 40.68\n",
        "eastLongitude = -74.03\n",
        "westLongitude = -73.92\n",
        "highlight_box = box(eastLongitude, southLatitude, westLongitude, northLatitude)\n",
        "highlight_gdf = (\n",
        "    gpd.GeoDataFrame({'geometry': [highlight_box]}, crs=\"EPSG:4326\")\n",
        "      .to_crs(epsg=3857)\n",
        ")\n",
        "\n",
        "# ── Load and project NYC\n",
        "nyc = gpd.read_file(get_path(\"nybb\")).to_crs(epsg=3857)\n",
        "\n",
        "# ── Convert centroids to GeoDataFrame\n",
        "#    `centroids` is your array of shape (n_clusters, 2) in [lon, lat]\n",
        "centroid_pts = gpd.GeoDataFrame(\n",
        "    geometry=[Point(lon, lat) for lon, lat in centroids],\n",
        "    crs=\"EPSG:4326\"\n",
        ").to_crs(epsg=3857)\n",
        "\n",
        "# ── Plot everything\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "nyc.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=1)\n",
        "highlight_gdf.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=3, linestyle='--')\n",
        "\n",
        "# Plot centroids\n",
        "centroid_pts.plot(\n",
        "    ax=ax,\n",
        "    marker='X',\n",
        "    color='blue',\n",
        "    markersize=150,\n",
        "    label='Cluster Centers'\n",
        ")\n",
        "\n",
        "ctx.add_basemap(ax, crs=nyc.crs, source=ctx.providers.CartoDB.Positron)\n",
        "\n",
        "ax.set_title(\"NYC with Taxi Rank Clusters & Highlighted Area\")\n",
        "ax.legend()\n",
        "ax.set_axis_off()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lC1R7xHoQHBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import box\n",
        "import matplotlib.pyplot as plt\n",
        "import contextily as ctx\n",
        "\n",
        "# ── 1) Define bbox and load/project NYC just once\n",
        "northLatitude = 40.86\n",
        "southLatitude = 40.68\n",
        "eastLongitude = -74.03\n",
        "westLongitude = -73.92\n",
        "\n",
        "# make box GeoDataFrame\n",
        "bbox = box(eastLongitude, southLatitude, westLongitude, northLatitude)\n",
        "bbox_gdf = gpd.GeoDataFrame({'geometry':[bbox]}, crs=\"EPSG:4326\").to_crs(epsg=3857)\n",
        "\n",
        "# your highlight box (same as bbox here)\n",
        "highlight_gdf = bbox_gdf.copy()\n",
        "\n",
        "# load NYC once and project\n",
        "nyc = gpd.read_file(get_path(\"nybb\")).to_crs(epsg=3857)\n",
        "\n",
        "# ── 2) Project all pickups to Web Mercator\n",
        "pickup_merc = pickup_points.to_crs(epsg=3857)\n",
        "\n",
        "# ── 3) Clip to the area of interest\n",
        "pickup_clip = gpd.clip(pickup_merc, bbox_gdf)\n",
        "nyc_clip    = gpd.clip(nyc,    bbox_gdf)\n",
        "\n",
        "# ── 4) Extract X/Y coords for density\n",
        "xs = pickup_clip.geometry.x.values\n",
        "ys = pickup_clip.geometry.y.values\n",
        "\n",
        "# choose number of bins for resolution\n",
        "nx, ny = 200, 200\n",
        "heatmap, xedges, yedges = np.histogram2d(xs, ys, bins=[nx, ny])\n",
        "\n",
        "# compute extent in map coords\n",
        "extent = [xedges.min(), xedges.max(), yedges.min(), yedges.max()]\n",
        "\n",
        "# ── 5) Plot clipped map + heatmap + box + centroids\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "# basemap + clipped street outlines\n",
        "nyc_clip.plot(ax=ax, facecolor='none', edgecolor='black', linewidth=0.5)\n",
        "\n",
        "# density heatmap\n",
        "ax.imshow(\n",
        "    heatmap.T,             # transpose to align x/y\n",
        "    extent=extent,         # put it in the right place\n",
        "    origin='lower',        # y-axis goes bottom→top\n",
        "    cmap='Reds',           # your heatmap palette\n",
        "    alpha=0.6\n",
        ")\n",
        "\n",
        "# highlight box border\n",
        "highlight_gdf.plot(\n",
        "    ax=ax,\n",
        "    facecolor='none',\n",
        "    edgecolor='blue',\n",
        "    linewidth=2,\n",
        "    linestyle='--',\n",
        "    label='Focus Area'\n",
        ")\n",
        "\n",
        "# optional: plot your KMeans centers\n",
        "centroid_pts.to_crs(epsg=3857).plot(\n",
        "    ax=ax,\n",
        "    marker='X',\n",
        "    color='white',\n",
        "    edgecolor='black',\n",
        "    markersize=100,\n",
        "    label='Cluster Centers'\n",
        ")\n",
        "\n",
        "# add basemap tiles under everything\n",
        "ctx.add_basemap(ax, crs=nyc.crs, source=ctx.providers.CartoDB.Positron)\n",
        "\n",
        "ax.set_title(\"Taxi Pickup Density & Cluster Centers\\nZoomed to 40.68–40.86 N, 74.03–73.92 W\")\n",
        "ax.legend()\n",
        "ax.set_axis_off()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jb-JFM_zSoKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NxAlWTB7QGuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE with code using cuML\n"
      ],
      "metadata": {
        "id": "EDzIB7_MENlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# COMPLETE with code using Spark Mlib\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans as SparkKMeans\n",
        "import time\n",
        "\n",
        "# Start Spark\n",
        "spark = SparkSession.builder.appName(\"TaxiKMeans\").getOrCreate()\n",
        "\n",
        "# Convert pandas DataFrame to Spark DataFrame\n",
        "spark_df = spark.createDataFrame(df)\n",
        "\n",
        "# Assemble features\n",
        "vec_assembler = VectorAssembler(\n",
        "    inputCols=[\"pickup_longitude\", \"pickup_latitude\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "spark_vec_df = vec_assembler.transform(spark_df)\n",
        "\n",
        "# Run KMeans\n",
        "start = time.time()\n",
        "kmeans = SparkKMeans(k=3, seed=42)\n",
        "model = kmeans.fit(spark_vec_df)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Spark MLlib KMeans execution time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Get cluster centers\n",
        "centroids_spark = model.clusterCenters()\n"
      ],
      "metadata": {
        "id": "VTCmQfwaFWt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Spark centroids to numpy array\n",
        "centroids_np = np.array(centroids_spark)\n",
        "\n",
        "# Plot heatmap with seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Heatmap of pickup density\n",
        "sns.kdeplot(\n",
        "    x=df[\"pickup_longitude\"],\n",
        "    y=df[\"pickup_latitude\"],\n",
        "    fill=True,\n",
        "    cmap=\"Reds\",\n",
        "    alpha=0.6,\n",
        "    bw_adjust=0.5\n",
        ")\n",
        "\n",
        "# Overlay the Spark KMeans centroids\n",
        "plt.scatter(\n",
        "    centroids_np[:, 0],  # longitudes\n",
        "    centroids_np[:, 1],  # latitudes\n",
        "    color='blue',\n",
        "    marker='X',\n",
        "    s=150,\n",
        "    label='Suggested Taxi Ranks (Spark KMeans)'\n",
        ")\n",
        "\n",
        "plt.title(\"Taxi Pickup Heatmap with Spark MLlib Cluster Centers\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t7_Qb1ArL1Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execution times:**\n",
        "\n",
        "scikit-learn: **TO COMPLETE**\n",
        "\n",
        "cuML: **TO COMPLETE**\n",
        "\n",
        "Spark Mlib: **TO COMPLETE**\n"
      ],
      "metadata": {
        "id": "RkfL4X68I5MW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "bccdda5f-23d5-4f86-ad4b-d7ac0da223e3",
          "showTitle": false,
          "title": ""
        },
        "id": "5GZgAV12XPBg"
      },
      "source": [
        "#### Discussion\n",
        "\n",
        "**COMPLETE: Explain the rationale of your solution, your code and discuss results**\n",
        "\n",
        "scikit-learn is optimized for small to medium-sized datasets and runs efficiently on a single machine. It's fast, easy to use, and integrates well with Python's data science ecosystem, making it ideal for local development, rapid prototyping, and small-scale tasks. On the other hand, Spark MLlib is built for large-scale data processing in distributed environments, making it suitable for big data workloads that can't fit into a single machine's memory. It leverages the power of cluster computing to handle massive datasets and is integrated into the larger Spark ecosystem, which is commonly used in production-level data pipelines. While scikit-learn excels in simplicity and speed for smaller tasks, Spark MLlib shines when dealing with large-scale data and distributed computing needs."
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookName": "Group project resolution - template final",
      "notebookOrigID": 149761708232030,
      "widgets": {}
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}